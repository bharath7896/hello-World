{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distractors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharath7896/hello-World/blob/feature/distractors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gzFsvWp_HFq",
        "colab_type": "text"
      },
      "source": [
        "# DISTRACTOR GENERATION THROUGH TEXT RANKING \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVX3fDc3MgKT",
        "colab_type": "text"
      },
      "source": [
        "## PROBLEM STATEMENT:\n",
        "\n",
        "We are given a pair of question q answer a and Distractors d and we have to generate best pair of distractors for the remaining data .\n",
        "\n",
        "for all q,a =>d in Range(i) where i is the number of question answer pair\n",
        "\n",
        "where :\n",
        "\n",
        "d = {d1,d2,d3},\n",
        "\n",
        "d ~ a\n",
        "\n",
        "d is related to q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn7joMo8UcnQ",
        "colab_type": "text"
      },
      "source": [
        "## MAPPING INTO ML-PROBLEM:\n",
        "\n",
        "* The given problem falls under Natural Language Processing(NLP) where we have to rank the distractors preserving the similarity relation between question and answer as well as answer and distractor.\n",
        "\n",
        "\n",
        "* The problem statement falls under category of Supervised Text-Ranking problem \n",
        "\n",
        "* Based on the semantic similarity relations between and the distractor and answer as well as question the distractor set with high similarity ranking parameters will be considered as the best set of distractors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12mhech6Ud59",
        "colab_type": "text"
      },
      "source": [
        "## APPROACH :\n",
        "\n",
        "For all given pair of question 'q' answer 'a' and distractors 'd' we are going to consider the following features :\n",
        "\n",
        "* Average-word2vec using NLTK - for embedding the text data\n",
        "* cosine similarity - for finding the similarity metrics between docs\n",
        "* Pos-tag similarity using NLTK - for structural similarity\n",
        "* Token similarity using wordnet - for common tokens or words\n",
        "* TFIDF - for finding most similar words in the documents\n",
        "* Length similarity - for finding the similarity between word count in d,a\n",
        "* Noun-phrase pos synonyms - for finding similar words for the NER\n",
        "* semantic similarity using Wordnet\n",
        "* EDIT distance = for number of edist needed to change the given d to a\n",
        " from the above features we are going to rank the dij for j ={1,2,3} in such a way that for a given i d = (di1>~di2>~di3) ~ a or d ~ a and d ~ q\n",
        " where(>,<,~ is in terms of ranking of semantic similarity)\n",
        "\n",
        " * After the featurization techniques we are going to use 2-stage ranking :\n",
        " 1. In the 1st stage we are going to use Logistic regression for learn-to-rank and feature reduction\n",
        "\n",
        " 2. In the 2nd stage we are going to use Randomforest for learn-to-rank to increasing the accuracy of the predictions. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi8axgCMYNlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from textgenrnn import textgenrnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv5nCN3WY4V8",
        "colab_type": "code",
        "outputId": "f9089ac9-e72f-4172-c46c-0b359af133bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjRolC0Bds5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading train and test files \n",
        "train_data = pd.read_csv('Train.csv')\n",
        "test_data = pd.read_csv('Test.csv')\n",
        "result_data = pd.read_csv('Results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHSJ-ga9VvB",
        "colab_type": "code",
        "outputId": "06be392a-609d-461a-e91c-42e3553e125a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Printing the shape of the train and test matrix\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31499, 3)\n",
            "(13500, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GktfJLpRdtMp",
        "colab_type": "code",
        "outputId": "760dc341-b36c-4d8d-d057-90b56bfe7daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# printing sample data from train\n",
        "train_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>distractor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Meals can be served</td>\n",
              "      <td>in rooms at 9:00 p. m.</td>\n",
              "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It can be inferred from the passage that</td>\n",
              "      <td>The local government can deal with the problem...</td>\n",
              "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The author called Tommy 's parents in order to</td>\n",
              "      <td>help them realize their influence on Tommy</td>\n",
              "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It can be inferred from the passage that</td>\n",
              "      <td>the writer is not very willing to use idioms</td>\n",
              "      <td>'idioms are the most important part in a langu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can we deal with snake wounds according to...</td>\n",
              "      <td>Stay calm and do n't move .</td>\n",
              "      <td>'Cut the wound and suck the poison out .'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What was the writer 's problem when she studie...</td>\n",
              "      <td>She missed her family very much .</td>\n",
              "      <td>\"She did n't like her new school .\", \"She did ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Who were killed on February 5 in a small town ...</td>\n",
              "      <td>Chen Jianqing and one of her partners</td>\n",
              "      <td>'Chen Jianqing and her husband', 'Chen Jingmin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>According to the writer , which of the followi...</td>\n",
              "      <td>Soccer is popular all over the world , but tru...</td>\n",
              "      <td>'Millions of people all over the world are pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>During a fire children often</td>\n",
              "      <td>panic</td>\n",
              "      <td>'know certain steps'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What 's the title of the passage ?</td>\n",
              "      <td>Five children died in a kindergarten bus accid...</td>\n",
              "      <td>'A bus accident in Deng zhou .', 'All primary ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  ...                                         distractor\n",
              "0                                Meals can be served  ...  'outside the room at 3:00 p. m.', 'in the dini...\n",
              "1           It can be inferred from the passage that  ...  'If some tragedies occur again ', ' relevant d...\n",
              "2     The author called Tommy 's parents in order to  ...  'blame Tommy for his failing grades', 'blame T...\n",
              "3           It can be inferred from the passage that  ...  'idioms are the most important part in a langu...\n",
              "4  How can we deal with snake wounds according to...  ...          'Cut the wound and suck the poison out .'\n",
              "5  What was the writer 's problem when she studie...  ...  \"She did n't like her new school .\", \"She did ...\n",
              "6  Who were killed on February 5 in a small town ...  ...  'Chen Jianqing and her husband', 'Chen Jingmin...\n",
              "7  According to the writer , which of the followi...  ...  'Millions of people all over the world are pla...\n",
              "8                       During a fire children often  ...                               'know certain steps'\n",
              "9                 What 's the title of the passage ?  ...  'A bus accident in Deng zhou .', 'All primary ...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDTjq7YweJQG",
        "colab_type": "code",
        "outputId": "2e931b9a-25ed-4221-e9c6-9dbb6e315ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# printing sample test data\n",
        "test_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What 'S the main idea of the text ?</td>\n",
              "      <td>The lack of career -- based courses in US high...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In the summer high season , Finland does nt se...</td>\n",
              "      <td>the sun is out at night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If you want to apply for Chinese Business Inte...</td>\n",
              "      <td>have to get confirmed at least twice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>That afternoon , the boy 's clothes were dry b...</td>\n",
              "      <td>nobody made room for him in the water .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which of the following statements is NOT true ?</td>\n",
              "      <td>There are twelve countries in the World Wildli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The problem of \" lock - in \" can be dangerous ...</td>\n",
              "      <td>it may make it difficult for customers to reco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The passage is mainly about</td>\n",
              "      <td>how Billy made blueberry juice with his uncle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Which of the following in not true ?</td>\n",
              "      <td>The snail 's teeth ca n't be worn out ..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What should you do at mealtime ?</td>\n",
              "      <td>Eat the food your host family gives you .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The part of \" Only you can make a card like th...</td>\n",
              "      <td>how to make a meaningful DIY card</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                                        answer_text\n",
              "0                What 'S the main idea of the text ?  The lack of career -- based courses in US high...\n",
              "1  In the summer high season , Finland does nt se...                            the sun is out at night\n",
              "2  If you want to apply for Chinese Business Inte...               have to get confirmed at least twice\n",
              "3  That afternoon , the boy 's clothes were dry b...            nobody made room for him in the water .\n",
              "4    Which of the following statements is NOT true ?  There are twelve countries in the World Wildli...\n",
              "5  The problem of \" lock - in \" can be dangerous ...  it may make it difficult for customers to reco...\n",
              "6                        The passage is mainly about      how Billy made blueberry juice with his uncle\n",
              "7               Which of the following in not true ?           The snail 's teeth ca n't be worn out ..\n",
              "8                   What should you do at mealtime ?          Eat the food your host family gives you .\n",
              "9  The part of \" Only you can make a card like th...                  how to make a meaningful DIY card"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwpp06LfijU",
        "colab_type": "code",
        "outputId": "d2f03f23-b7e2-418b-8437-45e65b076b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# checking for null values\n",
        "train_data.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question       0\n",
              "answer_text    0\n",
              "distractor     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KxgT3NJfiNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the text data in dataframe from Nonetype to string\n",
        "train_data['question'] =  train_data.question.astype('str') \n",
        "train_data['answer_text'] = train_data.answer_text.astype('str')\n",
        "train_data['distractor'] = train_data.distractor.astype('str')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08peHUCILK2B",
        "colab_type": "text"
      },
      "source": [
        "## TEXT PREPROCESSING "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adBAJDkY76P",
        "colab_type": "code",
        "outputId": "b5a2d877-4e24-4ae7-da58-a5059e9cba8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title Applying filtering of special charecters in the text and converting into plain text \n",
        "from tqdm import tqdm\n",
        "preprocessed_quesn = []\n",
        "# tqdm is for printing the status bar\n",
        "for phrase in tqdm(train_data['question']):\n",
        "  phrase = re.sub(r\"did n't\",'did not',phrase)\n",
        "  phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  phrase = phrase.replace('\\\\r', ' ')\n",
        "  phrase = phrase.replace('\\\\n', ' ')\n",
        "  phrase = re.sub(\"[^A-Za-z0-9.,]+\", ' ',phrase)\n",
        "  phrase = phrase.lower()\n",
        "  preprocessed_quesn.append(phrase)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31499/31499 [00:00<00:00, 76198.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZB0jQC9dMlt",
        "colab_type": "code",
        "outputId": "93169b79-d709-42d7-af5a-ced844cbded2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# printing sample of preprocessed question\n",
        "preprocessed_quesn[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it can be inferred from the passage that'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxTQS5z4vYTs",
        "colab_type": "code",
        "outputId": "69c65b3a-ca59-4c79-a9b6-35687e971cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title Preprocessing special charecters and converting into clean text\n",
        "from tqdm import tqdm\n",
        "preprocessed_ans = []\n",
        "# tqdm is for printing the status bar\n",
        "for phrase in tqdm(train_data['answer_text'].astype('str')):\n",
        "  phrase = re.sub(r\"did n't\",'did not',phrase)\n",
        "  phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  phrase = phrase.replace('\\\\r', ' ')\n",
        "  phrase = phrase.replace('\\\\n', ' ')\n",
        "  phrase = re.sub(\"[^A-Za-z0-9.,]+\", ' ',phrase)\n",
        "  phrase = phrase.lower()\n",
        "  preprocessed_ans.append(phrase)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31499/31499 [00:00<00:00, 77456.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCq8KFb7d417",
        "colab_type": "text"
      },
      "source": [
        "The distractor feature has set of 1-15 sentences which we need to preprocess and convert into set of 3  setences . Not only that it also contains repeated sentences and dis-joint sentences ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCxuZHF6xB5K",
        "colab_type": "code",
        "outputId": "abddc52b-1e7e-4787-9a35-3c8ff4ef5ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title modelling the sentences and removing unneccessary punctuations\n",
        "from nltk.tokenize import sent_tokenize\n",
        "dist_vect = []\n",
        "for phrase in tqdm(train_data['distractor']):\n",
        "  phrase = re.sub(r\"did n't\",'did not',phrase)\n",
        "  phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  phrase = phrase.replace('\\\\r', ' ')\n",
        "  phrase = phrase.replace('\\\\n', ' ')\n",
        "  phrase = re.sub(\"[^A-Za-z0-9.,]+\", ' ',phrase)\n",
        "  phrase = phrase.lower()\n",
        "  phrase = re.split(',',phrase)\n",
        "  dist_vect.append(phrase)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31499/31499 [00:00<00:00, 42492.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCFpOyRsy7kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filling the incomplete distractors with suitable values\n",
        "vel= []\n",
        "vale =[]\n",
        "for vale in dist_vect:\n",
        "  if len(vale)==1:\n",
        "    vale.append('Both the bove are correct')\n",
        "    vale.append('None of the above')\n",
        "  elif len(vale)==2:\n",
        "    vale.append('None of the above')\n",
        "  vel.append(vale)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bByiIgT5RfpI",
        "colab_type": "code",
        "outputId": "05d739b6-c33e-4438-c809-459858a2694d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# counting how many distractor list have distractors greater than 3\n",
        "for i in range(4,16):\n",
        "  t = len([id for id,v in enumerate(vel) if len(v)==i])\n",
        "  print(i,t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 1219\n",
            "5 318\n",
            "6 290\n",
            "7 41\n",
            "8 26\n",
            "9 23\n",
            "10 7\n",
            "11 1\n",
            "12 10\n",
            "13 0\n",
            "14 0\n",
            "15 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrvVz6byAkEv",
        "colab_type": "code",
        "outputId": "866a436b-e707-4721-efed-97fd5d880f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp = [i for i,v in enumerate(vel) if len(v)>5]\n",
        "print(len(temp))\n",
        "temp_df = pd.DataFrame()\n",
        "temp_df['qstn'] = preprocessed_quesn\n",
        "temp_df['ans'] = preprocessed_ans\n",
        "temp_df['options'] = [v for v in vel]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzlMriPne0-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropping rows with options more than 5\n",
        "temp_df = temp_df.drop(index=temp)\n",
        "train_data = train_data.drop(index=temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gRWwL1MXFKO",
        "colab_type": "code",
        "outputId": "c5b4c26e-7e43-44c6-825a-b2caba38ef84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31098, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J7phc5eHXeZ",
        "colab_type": "code",
        "outputId": "bc53a170-59b7-47e1-a085-47a267bc7285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# @title selecting values whose distractor set is in range of 4-5\n",
        "tem = [i for i,v in enumerate(temp_df['options']) if (len(v)>3 and len(v)<6)]\n",
        "tp_tem = pd.DataFrame(temp_df['options'].values[tem])\n",
        "tp_tem.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ if some tragedies occur again ,  relevant de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[ millions of people all over the world are pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ sun yukun had to change his residence status...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[ 1 ,  500 people died on titanic is maiden vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ if the project is completed ,  the world is ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  [ if some tragedies occur again ,  relevant de...\n",
              "1  [ millions of people all over the world are pl...\n",
              "2  [ sun yukun had to change his residence status...\n",
              "3  [ 1 ,  500 people died on titanic is maiden vo...\n",
              "4  [ if the project is completed ,  the world is ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZwRycWVb2jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clearing the duplicates in distractors\n",
        "from collections import Counter\n",
        "dl = []\n",
        "dup = lambda x : Counter(x)\n",
        "for i,tres in enumerate(tp_tem[0]):\n",
        "  dpi = dup(tres)\n",
        "  dd = [k for k,v in dpi.items() if(v==2)]\n",
        "  dl.append(dd)\n",
        "  dl = [j for j in dl if j]\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umPEzEk_kkxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting all text in sequence\n",
        "import itertools\n",
        "from itertools import chain\n",
        "ele = []\n",
        "for el in chain.from_iterable(dl):\n",
        "  ele.append(el)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-w4RMF6n-lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing duplicate from the text\n",
        "res = [] \n",
        "for i in temp_df['options']: \n",
        "  if i not in res: \n",
        "    res.append(i) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2chgFhqUENKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connecting the dissjoint sentences in distractors\n",
        "tt = tp_tem[0]\n",
        "con = [el for i,el in enumerate(tt)]\n",
        "\n",
        "def cat(lst):\n",
        "  if len(lst)==4:\n",
        "    l = sorted(lst)\n",
        "    m=[l[0]+l[1],l[2],l[-1]]\n",
        "    return m\n",
        "  if len(lst)==5:\n",
        "    k = sorted(lst)\n",
        "    a = k[0]+k[2]\n",
        "    b = k[1]+k[3]\n",
        "    if (k[0]+k[2])>(k[1]+k[3]) :\n",
        "      n = [a,b,k[-1]]\n",
        "      return n\n",
        "    else :\n",
        "      n = [a+k[1],k[2],k[-1]]\n",
        "      return n\n",
        "\n",
        "prep = []              \n",
        "for tp in tp_tem[0]:\n",
        "   pre = cat(tp)\n",
        "   prep.append(pre)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqHt6WryN3GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating and splitting into individual distractors \n",
        "temp_df['options'].iloc[tem] = prep\n",
        "d1 = [] # distractor 1\n",
        "d2 = [] # distractor 2\n",
        "d3 = [] # distractor 3\n",
        "for tm in temp_df['options']:\n",
        "  d1.append(tm[0])\n",
        "  d2.append(tm[1])\n",
        "  d3.append(tm[2])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1viGAeUkN3et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merging thee individual distractors\n",
        "temp_df['d1'] = d1\n",
        "temp_df['d2'] = d2\n",
        "temp_df['d3'] = d3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRaQM7J_Rkyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# joining the text from distractors\n",
        "d = []\n",
        "for i in range(0,len(temp_df)):  \n",
        "  f = d1[i] + \",\" + d2[i] + \",\" + d3[i]\n",
        "  d.append(f)\n",
        "\n",
        "temp_df['d'] = d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMVrEs0GLnF7",
        "colab_type": "text"
      },
      "source": [
        "## FEATURIZATION TECHNIQUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mdcp85r6ZZl",
        "colab_type": "code",
        "outputId": "9cec4875-0d11-443a-dd24-a46e2b344666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "def loadGloveModel(gloveFile):\n",
        "    print (\"Loading Glove Model\")\n",
        "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
        "    model = {}\n",
        "    for line in tqdm(f):\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "        model[word] = embedding\n",
        "    print (\"Done.\",len(model),\" words loaded!\")\n",
        "    return model\n",
        "model = loadGloveModel('glove.42B.300d.txt')\n",
        "words = []\n",
        "for i in preproced_texts:\n",
        "    words.extend(i.split(' '))\n",
        "\n",
        "for i in preproced_titles:\n",
        "    words.extend(i.split(' '))\n",
        "print(\"all the words in the coupus\", len(words))\n",
        "words = set(words)\n",
        "print(\"the unique words in the coupus\", len(words))\n",
        "\n",
        "inter_words = set(model.keys()).intersection(words)\n",
        "print(\"The number of words that are present in both glove vectors and our coupus\", \\\n",
        "      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n",
        "\n",
        "words_courpus = {}\n",
        "words_glove = set(model.keys())\n",
        "for i in words:\n",
        "    if i in words_glove:\n",
        "        words_courpus[i] = model[i]\n",
        "print(\"word 2 vec length\", len(words_courpus))\n",
        "\n",
        "\n",
        "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
        "\n",
        "import pickle\n",
        "with open('glove_vectors', 'wb') as f:\n",
        "    pickle.dump(words_courpus, f)\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef loadGloveModel(gloveFile):\\n    print (\"Loading Glove Model\")\\n    f = open(gloveFile,\\'r\\', encoding=\"utf8\")\\n    model = {}\\n    for line in tqdm(f):\\n        splitLine = line.split()\\n        word = splitLine[0]\\n        embedding = np.array([float(val) for val in splitLine[1:]])\\n        model[word] = embedding\\n    print (\"Done.\",len(model),\" words loaded!\")\\n    return model\\nmodel = loadGloveModel(\\'glove.42B.300d.txt\\')\\nwords = []\\nfor i in preproced_texts:\\n    words.extend(i.split(\\' \\'))\\n\\nfor i in preproced_titles:\\n    words.extend(i.split(\\' \\'))\\nprint(\"all the words in the coupus\", len(words))\\nwords = set(words)\\nprint(\"the unique words in the coupus\", len(words))\\n\\ninter_words = set(model.keys()).intersection(words)\\nprint(\"The number of words that are present in both glove vectors and our coupus\",       len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\\n\\nwords_courpus = {}\\nwords_glove = set(model.keys())\\nfor i in words:\\n    if i in words_glove:\\n        words_courpus[i] = model[i]\\nprint(\"word 2 vec length\", len(words_courpus))\\n\\n\\n# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\\n\\nimport pickle\\nwith open(\\'glove_vectors\\', \\'wb\\') as f:\\n    pickle.dump(words_courpus, f)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAi7PPIQ28Lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure you have the glove_vectors file\n",
        "with open('glove_vectors', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    glove_words =  set(model.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKsdxQEl5pNH",
        "colab_type": "code",
        "outputId": "fe5e0122-6bf6-4931-b6f1-c63deeb60fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "q_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored questions\n",
        "for sentence in tqdm(temp_df['qstn']): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # @title num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if word in glove_words:\n",
        "            vector += model[word]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vector /= cnt_words\n",
        "    q_avg_w2v_vectors.append(vector)\n",
        "\n",
        "print(len(q_avg_w2v_vectors))\n",
        "print(len(q_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 41983.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTOqQChe8Zn7",
        "colab_type": "code",
        "outputId": "7228c703-3ce4-4c31-a670-1366c9f7c6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ans_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in answers\n",
        "for sentences in tqdm(temp_df['ans']): # for each review/sentence\n",
        "    vectors = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for words in sentence.split(): # for each word in a review/sentence\n",
        "        if words in glove_words:\n",
        "            vectors += model[words]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vectors /= cnt_words\n",
        "    ans_avg_w2v_vectors.append(vector)\n",
        "\n",
        "print(len(ans_avg_w2v_vectors))\n",
        "print(len(ans_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 39812.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXGXR84bUp19",
        "colab_type": "code",
        "outputId": "20f6c6a4-225e-415b-f990-d705ba1ce44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "d1_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in 1st distractor set\n",
        "for sent in tqdm(temp_df['d1']): # for each review/sentence\n",
        "    vect = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_wor =0; # num of words with a valid vector in the sentence/review\n",
        "    for wor in sent.split(): # for each word in a review/sentence\n",
        "        if wor in glove_words:\n",
        "            vect += model[wor]\n",
        "            cnt_wor += 1\n",
        "    if cnt_wor != 0:\n",
        "        vect /= cnt_wor\n",
        "    d1_avg_w2v_vectors.append(vect)\n",
        "\n",
        "print(len(d1_avg_w2v_vectors))\n",
        "print(len(d1_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 42280.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xshK1GSLU3SD",
        "colab_type": "code",
        "outputId": "3af264b2-c426-4af1-ef28-878b5896cb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "d2_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in 2nd distractor\n",
        "for senten in tqdm(temp_df['d2']): # for each review/sentence\n",
        "    vecto = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_worde =0; # num of words with a valid vector in the sentence/review\n",
        "    for worde in senten.split(): # for each word in a review/sentence\n",
        "        if worde in glove_words:\n",
        "            vecto += model[worde]\n",
        "            cnt_worde += 1\n",
        "    if cnt_worde != 0:\n",
        "        vecto /= cnt_worde\n",
        "    d2_avg_w2v_vectors.append(vect)\n",
        "\n",
        "print(len(d2_avg_w2v_vectors))\n",
        "print(len(d2_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 49523.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUHAk0GxU4Ph",
        "colab_type": "code",
        "outputId": "b67ce373-fc25-4612-cc3b-88efedf2311f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "d3_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in 3rd distractor set\n",
        "for sentenc in tqdm(temp_df['d3']): # for each review/sentence\n",
        "    vec = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_wordes =0; # num of words with a valid vector in the sentence/review\n",
        "    for wordes in sentenc.split(): # for each word in a review/sentence\n",
        "        if wordes in glove_words:\n",
        "            vec += model[wordes]\n",
        "            cnt_wordes += 1\n",
        "    if cnt_wordes != 0:\n",
        "        vec /= cnt_wordes\n",
        "    d3_avg_w2v_vectors.append(vec)\n",
        "\n",
        "print(len(d3_avg_w2v_vectors))\n",
        "print(len(d3_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 54834.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q42iifJ_HXvo",
        "colab_type": "code",
        "outputId": "ed92dae4-f26f-4b3f-80da-de166c03efd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "d_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in total distractor set\n",
        "for sentences in tqdm(temp_df['d']): # for each review/sentence\n",
        "    vectors = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for words in sentence.split(): # for each word in a review/sentence\n",
        "        if words in glove_words:\n",
        "            vectors += model[words]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vectors /= cnt_words\n",
        "    d_avg_w2v_vectors.append(vectors)\n",
        "\n",
        "print(len(d_avg_w2v_vectors))\n",
        "print(len(d_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31098/31098 [00:00<00:00, 42446.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31098\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W5LsAsUkHId",
        "colab_type": "code",
        "outputId": "295f2b15-2d02-47eb-a8a0-da525fff01ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "qt_avg_w2v_vectors = []; # @title the avg-w2v for each sentence/review is stored in test question\n",
        "for sentence in tqdm(test_data['question']): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_word =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if word in glove_words:\n",
        "            vector += model[word]\n",
        "            cnt_word += 1\n",
        "    if cnt_word != 0:\n",
        "        vector /= cnt_word\n",
        "    qt_avg_w2v_vectors.append(vector)\n",
        "\n",
        "print(len(qt_avg_w2v_vectors))\n",
        "print(len(qt_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13500/13500 [00:00<00:00, 45897.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13500\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQbX8dzIwy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title computing cosine similarity of two text documents\n",
        "import math\n",
        "def cosine_similar(v1,v2):\n",
        "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i]; y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)\n",
        "\n",
        "csad = [cosine_similar(ans_avg_w2v_vectors[i],d_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHefsQIbo-bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csaq = [cosine_similar(ans_avg_w2v_vectors[i],q_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csd1q = [cosine_similar(q_avg_w2v_vectors[i],d1_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csd2q = [cosine_similar(q_avg_w2v_vectors[i],d2_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csd3q = [cosine_similar(q_avg_w2v_vectors[i],d3_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csad1 = [cosine_similar(d1_avg_w2v_vectors[i],ans_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csad2 = [cosine_similar(d2_avg_w2v_vectors[i],ans_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n",
        "csad3 = [cosine_similar(ans_avg_w2v_vectors[i],d3_avg_w2v_vectors[i]) for i in range(0,len(d_avg_w2v_vectors))]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ2XqmmlHKs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the lengths of the text in the fields\n",
        "a = [len(ele) for ele in test_data['answer_text']]\n",
        "b = {len(ale):t for t,ale in enumerate(d1)}\n",
        "c = {len(ela):s for s,ela in enumerate(d2)}\n",
        "d = {len(eal):r for r,eal in enumerate(d3)}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWeWFp0nVw92",
        "colab_type": "code",
        "outputId": "407adc9b-cf8b-4705-83aa-fd670b91ac58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "at_avg_w2v_vectors = []; # @title the avg-w2v for each sentence is stored in this list\n",
        "for sentences in tqdm(test_data['answer_text']): # for each review/sentence\n",
        "    vectors = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for words in sentence.split(): # for each word in a review/sentence\n",
        "        if words in glove_words:\n",
        "            vectors += model[words]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vectors /= cnt_words\n",
        "    at_avg_w2v_vectors.append(vectors)\n",
        "\n",
        "print(len(at_avg_w2v_vectors))\n",
        "print(len(at_avg_w2v_vectors[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13500/13500 [00:00<00:00, 49618.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13500\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKstMX3-eQBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tcsaq = [cosine_similar(at_avg_w2v_vectors[i],qt_avg_w2v_vectors[i]) for i in range(0,len(at_avg_w2v_vectors))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UUQEwRXYQx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tcsad1 = [cosine_similar(at_avg_w2v_vectors[i],d1_avg_w2v_vectors[i]) for i in range(0,len(at_avg_w2v_vectors))]\n",
        "tcsad2 = [cosine_similar(at_avg_w2v_vectors[i],d2_avg_w2v_vectors[i]) for i in range(0,len(at_avg_w2v_vectors))]\n",
        "tcsad3 = [cosine_similar(at_avg_w2v_vectors[i],d3_avg_w2v_vectors[i]) for i in range(0,len(at_avg_w2v_vectors))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQHKMYP3vQ6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# appending the values to corresponding list\n",
        "dt1 = []\n",
        "dt2 = []\n",
        "dt3 = []\n",
        "for i in range(0,len(at_avg_w2v_vectors)):\n",
        "  t1 = d1[i]\n",
        "  dt1.append(t1)\n",
        "  t2 = d2[i]\n",
        "  dt2.append(t2)\n",
        "  t3 = d3[i]\n",
        "  dt3.append(t3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-UUBMyw_I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the dictionary out of the index values and text for hashing\n",
        "td1 = {k:v for v in tcsad1 for k in dt1}\n",
        "td2 = {a:b for b in tcsad2 for a in dt2}\n",
        "td3 = {p:q for q in tcsad3 for p in dt3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0c0XtbrzJHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary with reference\n",
        "td = dict()\n",
        "td.update(td1)\n",
        "td.update(td2)\n",
        "td.update(td3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTs8q7xCV-mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the negative or less raking values data\n",
        "test_negative = tcsad1 or tcsad2 or tcsad3\n",
        "test1 = tcsad1 and tcsad2\n",
        "test2 = test1 and tcsad3\n",
        "test_positive1 = test1 and test2\n",
        "test_positive2 = test1 or test2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDvhEjPUi601",
        "colab_type": "code",
        "outputId": "2d8109b5-887b-42ad-dace-265c84c805de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# @title pos-tagging similarity the text for structural analysis\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize\n",
        "w2va = temp_df['ans'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "w2vd = temp_df['d'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "w2vd1 = temp_df['d1'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "w2vd2 = temp_df['d2'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "w2vd3 = temp_df['d3'].apply(lambda x: pos_tag(word_tokenize(x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6yQ1zF21Utb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posans = list(w2va)\n",
        "posd = list(w2vd)\n",
        "posd1 = list(w2vd1)\n",
        "posd2 = list(w2vd2)\n",
        "posd3 = list(w2vd3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhHkpF8ZK29m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dd1 = dict(zip(csad1,d1))\n",
        "dd2 = dict(zip(csad2,d2))\n",
        "dd3 = dict(zip(csad3,d3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X11Az-S8s0tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['q'] = csaq\n",
        "df['a'] = csad1\n",
        "df['b'] = csad2\n",
        "df['c'] = csad3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41XALvTRuacz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the positive or high ranked text\n",
        "positive = [csad1 and csad2 and csad3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIn2lN7pwbbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg1 = [csad1 or csad2]\n",
        "neg2 = [csad3 or d]\n",
        "negative  =  neg1.append(neg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naqRqFFZlfpF",
        "colab_type": "code",
        "outputId": "487e1c53-fb59-41e8-a3a1-2b5c19294b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Tfidf for frequency count\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "ans_tfidf = vectorizer.fit_transform(temp_df['ans'])\n",
        "print(\"Shape of matrix after one hot encodig \",ans_tfidf.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of matrix after one hot encodig  (31098, 2570)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjUNLqfAvxAF",
        "colab_type": "code",
        "outputId": "c0d4d645-76a3-4304-c801-1560aa9a3475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tfidf on d1 \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "d1_tfidf = vectorizer.fit_transform(temp_df['d1'])\n",
        "print(\"Shape of matrix after one hot encodig \",d1_tfidf.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of matrix after one hot encodig  (31098, 2604)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swzFZt9ZXvv_",
        "colab_type": "code",
        "outputId": "cf3dd6bb-09fa-4186-f2d1-984881761438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tfidf on d2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "d2_tfidf = vectorizer.fit_transform(temp_df['d2'])\n",
        "print(\"Shape of matrix after one hot encodig \",d2_tfidf.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of matrix after one hot encodig  (31098, 2040)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqfUE-efXxj3",
        "colab_type": "code",
        "outputId": "4c152941-847f-4a91-f5ca-0fa039c736bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tfidf on d3\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "d3_tfidf = vectorizer.fit_transform(temp_df['d3'])\n",
        "print(\"Shape of matrix after one hot encodig \",d3_tfidf.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of matrix after one hot encodig  (31098, 1437)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVARfjYoXwet",
        "colab_type": "code",
        "outputId": "06ae3713-8fe9-4a69-80d1-058971c8c59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tfidf on questions\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "q_tfidf = vectorizer.fit_transform(temp_df['qstn'])\n",
        "print(\"Shape of matrix after one hot encodig \",q_tfidf.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of matrix after one hot encodig  (31098, 1758)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DNKFqo9AM3Q",
        "colab_type": "code",
        "outputId": "06667a68-2f14-4b79-fa87-4c53c38e4c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title  Edit Distance on answer text\n",
        "import editdistance\n",
        "ed1a = []\n",
        "ed2a = []\n",
        "ed3a = []\n",
        "for i in range(0,len(d2)):\n",
        "  ea1 = editdistance.eval(temp_df['ans'].values[i],d1[i])\n",
        "  ed1a.append(ea1)\n",
        "  ea2 = editdistance.eval(temp_df['ans'].values[i],d2[i])\n",
        "  ed2a.append(ea2)\n",
        "  ea3 = editdistance.eval(temp_df['ans'].values[i],d3[i])\n",
        "  ed3a.append(ea3)\n",
        "  \n",
        "print(len(ed1a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5q6Ddu9w_Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length of sentences\n",
        "def length(s1,s2):\n",
        "  l=len(s1)-len(s2)\n",
        "  return l\n",
        "lenad1 = []\n",
        "lenad2 = []\n",
        "lenad3 = []\n",
        "for i in range(0,len(d1)):\n",
        "  lad1 = length(temp_df['ans'].values[i],d1[i])\n",
        "  lenad1.append(lad1)\n",
        "  lad2 = length(temp_df['ans'].values[i],d2[i])\n",
        "  lenad2.append(lad2)\n",
        "  lad3 = length(temp_df['ans'].values[i],d3[i])\n",
        "  lenad3.append(lad3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enGhGtJ3byYU",
        "colab_type": "code",
        "outputId": "8f8d08a7-edca-4923-dcf3-458f662c6256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title Sequence matching score\n",
        "import difflib\n",
        "def seqm(a,b):\n",
        "  seq = difflib.SequenceMatcher(None,a,b)\n",
        "  c = seq.ratio()\n",
        "  return c\n",
        "\n",
        "seqad1 = []\n",
        "seqad2 = []\n",
        "seqad3 = []\n",
        "for i in range(0,len(temp_df)):\n",
        "  sad1 = seqm(preprocessed_ans[i],d1[i])\n",
        "  seqad1.append(sad1)\n",
        "  sad2 = seqm(preprocessed_ans[i],d2[i])\n",
        "  seqad2.append(sad2)\n",
        "  sad3 = seqm(preprocessed_ans[i],d3[i])\n",
        "  seqad3.append(sad3)\n",
        "len(seqad3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31098"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0eScOwNb7RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Sentence similarity using wordnet\n",
        "def length_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of the length of the shortest path in the semantic\n",
        "    ontology (Wordnet in our case as well as the paper's) between two\n",
        "    synsets.\n",
        "    \"\"\"\n",
        "    l_dist = sys.maxint\n",
        "    if synset_1 is None or synset_2 is None:\n",
        "        return 0.0\n",
        "    if synset_1 == synset_2:\n",
        "        # if synset_1 and synset_2 are the same synset return 0\n",
        "        l_dist = 0.0\n",
        "    else:\n",
        "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])\n",
        "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
        "        if len(wset_1.intersection(wset_2)) > 0:\n",
        "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
        "            l_dist = 1.0\n",
        "        else:\n",
        "            # just compute the shortest path between the two\n",
        "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
        "            if l_dist is None:\n",
        "                l_dist = 0.0\n",
        "    # normalize path length to the range [0,1]\n",
        "    return math.exp(-alpha * l_dist)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shi5lAUeClo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title noun phrase pos synonyms using wordnet\n",
        "def length_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of the length of the shortest path in the semantic\n",
        "    ontology (Wordnet in our case as well as the paper's) between two\n",
        "    synsets.\n",
        "    \"\"\"\n",
        "    l_dist = sys.maxint\n",
        "    if synset_1 is None or synset_2 is None:\n",
        "      return 0.0\n",
        "    if synset_1 == synset_2:\n",
        "      # if synset_1 and synset_2 are the same synset return 0\n",
        "      l_dist = 0.0\n",
        "    else:\n",
        "      wset_1 = set([str(x.name()) for x in synset_1.lemmas()])\n",
        "      wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
        "      if len(wset_1.intersection(wset_2)) > 0:\n",
        "        # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
        "        l_dist = 1.0\n",
        "      else:\n",
        "        # just compute the shortest path between the two\n",
        "        l_dist = synset_1.shortest_path_distance(synset_2)\n",
        "        if l_dist is None:\n",
        "          l_dist = 0.0\n",
        "  # normalize path length to the range [0,1]\n",
        "    return math.exp(-alpha * l_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q-CzZADClKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Q-A analysis using wordnet\n",
        "def _analyze_query(self):\n",
        "  tagged = nltk.pos_tag(self.ir_query)\n",
        "  ir_query_tagged = []\n",
        "  for word, pos in tagged:\n",
        "    pos = {pos.startswith('N'): wordnet.NOUN,\n",
        "          pos.startswith('V'): wordnet.VERB,\n",
        "          pos.startswith('J'): wordnet.ADJ,\n",
        "          pos.startswith('R'): wordnet.ADV,}.get(pos, None)\n",
        "    if pos:\n",
        "      synsets = wordnet.synsets(word, pos=pos)\n",
        "    else:\n",
        "      synsets = wordnet.synsets(word)\n",
        "    ir_query_tagged.append((word, synsets))\n",
        "            \n",
        "  # Add additional special hidden term\n",
        "  ir_query_tagged.append(('cause', [wordnet.synset('cause.v.01')]))\n",
        "  self.ir_query_tagged = ir_query_tagged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS_212Au4ZXA",
        "colab_type": "code",
        "outputId": "ef0d361c-fd3c-41f7-d272-54500178862a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# @title short sentence similarity\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_best_synset_pair(word_1, word_2):\n",
        "    \"\"\"\n",
        "    Choose the pair with highest path similarity among all pairs.\n",
        "    Mimics pattern-seeking behavior of humans.\n",
        "    \"\"\"\n",
        "    max_sim = -1.0\n",
        "    synsets_1 = wn.synsets(word_1)\n",
        "    synsets_2 = wn.synsets(word_2)\n",
        "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
        "        return None, None\n",
        "    else:\n",
        "        max_sim = -1.0\n",
        "        best_pair = None, None\n",
        "        for synset_1 in synsets_1:\n",
        "            for synset_2 in synsets_2:\n",
        "               sim = wn.path_similarity(synset_1, synset_2)\n",
        "               if sim is not None and sim > max_sim:\n",
        "                   max_sim = sim\n",
        "                   best_pair = synset_1, synset_2\n",
        "        return best_pair"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLKXYYZ6uZ_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_q = list(test_data['question'])\n",
        "test_a = list(test_data['answer_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Aj2v4ZubGk",
        "colab_type": "code",
        "outputId": "54508cfa-6c39-409b-c37c-399bc8b273b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# @title Token similarity using nltk\n",
        "import nltk.corpus\n",
        "import nltk.tokenize.punkt\n",
        "import nltk.stem.snowball\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(string.punctuation)\n",
        "stopwords.append('')\n",
        "def token_set_match(a, b, threshold=0.5):\n",
        "    \"\"\"Check if a and b share token.\"\"\"\n",
        "    tokens_a = [token.lower().strip(string.punctuation) for token in word_tokenize(a) \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "    tokens_b = [token.lower().strip(string.punctuation) for token in word_tokenize(b) \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
        "    if ratio >= threshold:\n",
        "      return 1\n",
        "    else :\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKWmtJXOHkt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mda = []\n",
        "md1a = []\n",
        "md2a = []\n",
        "md3a = []\n",
        "mqd = []\n",
        "md1a = []\n",
        "md2a = []\n",
        "md3a = []\n",
        "mqd1 = []\n",
        "mqd2 = []\n",
        "mqd3 = []\n",
        "for i in range(0,len(temp_df)):\n",
        "  matchd1a = token_set_match(d1[i],temp_df['ans'].values[i])\n",
        "  md1a.append(matchd1a)\n",
        "  matchd2a = token_set_match(d2[i],temp_df['ans'].values[i])\n",
        "  md2a.append(matchd2a)\n",
        "  matchd3a = token_set_match(d3[i],temp_df['ans'].values[i])\n",
        "  md3a.append(matchd3a)\n",
        "  matchqd1 = token_set_match(temp_df['qstn'].values[i],d1[i])\n",
        "  mqd1.append(matchqd1)\n",
        "  matchqd2 = token_set_match(temp_df['qstn'].values[i],d2[i])\n",
        "  mqd2.append(matchqd2)\n",
        "  matchqd3 = token_set_match(temp_df['qstn'].values[i],d3[i])\n",
        "  mqd3.append(matchqd3)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdBV5xD2uZ1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame()\n",
        "train['csad1'] = csad1\n",
        "train['csad2'] = csad2\n",
        "train['csad3'] = csad3\n",
        "train['csd1q'] = csd1q\n",
        "train['csd2q'] = csd2q\n",
        "train['csd3q'] = csd3q\n",
        "train['csaq'] = csaq\n",
        "train['edit_d1a'] = ed1a\n",
        "train['edit_d2a'] = ed2a\n",
        "train['edit_d3a'] = ed3a\n",
        "train['length_ad1'] = lenad1\n",
        "train['length_ad2'] = lenad2\n",
        "train['length_ad3'] = lenad3\n",
        "train['seq_ad1'] = seqad1\n",
        "train['seq_ad2'] = seqad2\n",
        "train['seq_ad3'] = seqad3\n",
        "train['md1a'] = md1a\n",
        "train['md2a'] = md2a\n",
        "train['md3a'] = md3a\n",
        "train['mqd1'] = mqd1\n",
        "train['mqd2'] = mqd2\n",
        "train['mqd3'] = mqd3\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y7qYjs6AszO",
        "colab_type": "code",
        "outputId": "a4a5deba-561a-4926-c464-9cc8554f4496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31098, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WRzik-2kD49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filling nan values\n",
        "train = train.fillna(0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agwLdJXiPkgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train.drop(['csad3'], axis=1)\n",
        "Y = train['csad3']\n",
        "Y = Y.astype(int)\n",
        "Y = Y.values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBrj-ba1MG4x",
        "colab_type": "code",
        "outputId": "e0824a7c-9fa7-4215-97f1-e5e5ab164d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train,Y_train =  X,Y\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(\"=\"*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31098, 21) (31098, 1)\n",
            "====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7752aFsG1TDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data['csaq'] = tcsaq\n",
        "X_test = test_data['csaq'].values.reshape(-1,1) \n",
        "X_test = pd.DataFrame(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUiQe1DMVpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taking opposite of answer as one distractor\n",
        "tesd1 = []\n",
        "for phrase in test_data['answer_text']:\n",
        "  phrase = re.sub(r\"did n't\",'did',phrase)\n",
        "  phrase = re.sub(r\"won't\", \"will\", phrase)\n",
        "  phrase = re.sub(r\"can\\'t\", \"can\", phrase)\n",
        "  phrase = re.sub(r\" not \", \" \", phrase)\n",
        "  phrase = re.sub(r\" was \", \" was not\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is not\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will not\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have not\", phrase)\n",
        "  phrase = re.sub(r\"dis\\w+\", \" \",phrase)\n",
        "  phrase = re.sub(r\"un\\w+\" , \" \", phrase)\n",
        "  phrase = re.sub(r\"in\\w+\" , \" \", phrase) \n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  tesd1.append(phrase.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgIjdHhW9aTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tp1 = []\n",
        "tp2 = []\n",
        "for i in range(0,len(test_data)):\n",
        "  tp1.append(d3[i])\n",
        "  tp2.append(d2[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DECOnMuO5i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tespos  = pd.DataFrame()\n",
        "tespos['test_positive1'] = tp1\n",
        "tespos['test_positive2'] = tp2\n",
        "tespos['test_positive3'] = tesd1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXZ_CjH9MDnC",
        "colab_type": "text"
      },
      "source": [
        "## MODELLING (LEARN TO RANK)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njtt4RO1uZkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Applying text to rank using Logistic Regression \n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.utils import shuffle\n",
        "def train_model(model, prediction_function, X_train, y_train, X_test):\n",
        "  model.partial_fit(X_train, y_train)\n",
        "  y_train_pred = prediction_function(model, X_train)\n",
        "  print('train precision: ' + str(precision_score(y_train, y_train_pred)))\n",
        "  print('train recall: ' + str(recall_score(y_train, y_train_pred)))\n",
        "  print('train accuracy: ' + str(accuracy_score(y_train, y_train_pred)))\n",
        "  y_test_pred = prediction_function(model, X_test)\n",
        "  return model\n",
        "def get_predicted_outcome(model, data):\n",
        "    return np.argmax(model.predict(data), axis=1).astype(np.float32)\n",
        "def get_predicted_rank(model, data):\n",
        "    return model.predict_proba(data)[:, 1]\n",
        "\n",
        "clf1 = train_model(SGDRegressor(), get_predicted_outcome, X_train, Y_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkaSlV-VdWYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Applying learn to rank using RandomForest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def train_model(model, prediction_function, X_train, y_train, X_test):\n",
        "  model.partial_fit(X_train, y_train)\n",
        "  y_train_pred = prediction_function(model, X_train)\n",
        "  print('train precision: ' + str(precision_score(y_train, y_train_pred)))\n",
        "  print('train recall: ' + str(recall_score(y_train, y_train_pred)))\n",
        "  print('train accuracy: ' + str(accuracy_score(y_train, y_train_pred)))\n",
        "  y_test_pred = prediction_function(model, X_test)\n",
        "  return model\n",
        "def get_predicted_outcome(model, data):\n",
        "    return np.argmax(model.predict(data), axis=1).astype(np.float32)\n",
        "def get_predicted_rank(model, data):\n",
        "    return model.predict_proba(data)\n",
        "\n",
        "clf2 = train_model(RandomForestClassifier(), get_predicted_outcome, X_train, Y_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWLj0OdDe3A4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combining the best evaluated rank for the test distractors\n",
        "test_positive1 = [clf1[0] or clf1[0]]\n",
        "test_positive2 = [clf2[1] or clf1[1]]\n",
        "test_positive1 = [clf2[2] or clf1[2]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrgjMU5EIj_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title combining all the options as mentioned using sepertor or\n",
        "test_data['distractor'] = tespos.test_positive1.map(str) + \" or \" + tespos.test_positive2.map(str) + \" or \" +  tespos.test_positive3.map(str) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4arBf80juI7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del test_data['csaq']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx0fFgxUt85h",
        "colab_type": "code",
        "outputId": "15af001a-a1ec-405d-ac95-1bef198f6d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Printing the sample data of the ranked distractors\n",
        "test_data['distractor'].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     in the dining room from 7 30 a. m. to 9 15 p....\n",
              "1     the central government has established sound ...\n",
              "2    None of the above or  blame tommy for his fail...\n",
              "3     nothere are no ways to master idioms  or  non...\n",
              "4    None of the above or Both the bove are correct...\n",
              "Name: distractor, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5u5M7GuXBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# storing the predictions in csv file \n",
        "test_data.to_csv('predictions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cQJq9J-MOaj",
        "colab_type": "text"
      },
      "source": [
        "## CONCLUSION:\n",
        "\n",
        "THE TEXT WITH SIMILAR LENGTHS AND SIMILAR SEMANTICS WITH ALL THE ABOVE FEATURES WERE SELECTED WITH SIMILARITY OF 0.6 IN THE LEADERBOARD SCORE\n"
      ]
    }
  ]
}